{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "em4_-_2iM0dF"
   },
   "source": [
    "# 2- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "\n",
    "In this second notebook on sequence-to-sequence models using PyTorch and TorchText, we'll be implementing the model from [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078). This model will achieve improved test perplexity whilst only using a single layer RNN in both the encoder and the decoder. [[Tutorial](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb)]\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "Pretty much the same way as in tut1, though in this paper, the source sentences are not reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16734,
     "status": "ok",
     "timestamp": 1592439997085,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "6Hw_7iuPMqIe",
    "outputId": "63d3134e-5beb-46c6-bb70-e9c1c7a968dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /home/ammar/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (45.2.0.post20200210)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ammar/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ammar/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ammar/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ammar/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/ammar/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ammar/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/ammar/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/home/ammar/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: de_core_news_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz#egg=de_core_news_sm==2.3.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from de_core_news_sm==2.3.0) (2.3.2)\n",
      "Requirement already satisfied: setuptools in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (45.2.0.post20200210)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.41.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.18.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ammar/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.22.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/ammar/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ammar/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ammar/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ammar/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ammar/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2019.11.28)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ammar/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/ammar/anaconda3/lib/python3.7/site-packages/de_core_news_sm -->\n",
      "/home/ammar/anaconda3/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n",
      "<spacy.lang.en.English object at 0x7f3583ba5e10>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Hello. how do you do good, sir!\n",
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
      "Unique token in SRC (de) vocab: 7854\n",
      "Unique token in SRC (en) vocab: 5893\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "sandbox_path = \"models/\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_de = spacy.load('de')\n",
    "print(spacy_en)\n",
    "\n",
    "def tokenize_de(text):\n",
    "  \"Tokenize a German language string\"\n",
    "  # Not reversing for this one\n",
    "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "  return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "foo = spacy_en.tokenizer(\"Hello. how do you do good, sir!\")\n",
    "print(type(foo))\n",
    "print(foo)\n",
    "[type(t.text) for t in foo]\n",
    "\n",
    "SRC = Field(tokenize = tokenize_de, init_token='<sos>', \n",
    "            eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize = tokenize_en, init_token='<sos>', \n",
    "            eos_token='<eos>', lower=True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "print(f\"Unique token in SRC (de) vocab: {len(SRC.vocab)}\")\n",
    "print(f\"Unique token in SRC (en) vocab: {len(TRG.vocab)}\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfldkFgOrLtO"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Single layer GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16727,
     "status": "ok",
     "timestamp": 1592439997091,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "XcRByKeXM0DQ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, dropout_p):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim) # no dropout as only 1 layer\n",
    "    self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, src):\n",
    "    # src shp: [src_len, batch_sz]\n",
    "    embedded = self.dropout(self.embedding(src)) # shp: [src_len, batch_sz, emb_dim]\n",
    "    outputs, hidden = self.rnn(embedded)\n",
    "    # outputs shp: [src_len, batch_sz, hid_dim]\n",
    "    # outputs are always from the top hidden layer\n",
    "    # hidden shp: [1, batch_sz, hid_dim]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4tD1TvCo86h"
   },
   "source": [
    "## Decoder\n",
    "The Decoder is where the implementation differs from the original Seq2Seq from tut-1.\n",
    "\n",
    "Instead of the GRU in the decoder taking just the embedded target token, $d(y_t)$ and the previous hidden state $s_{t-1}$ as inputs, it also takes the context vector $z$.\n",
    "\n",
    "$$s_t = \\text{DecoderGRU}(d(y_t), s_{t-1}, z)$$\n",
    "Note how this context vector, $z$, does not have a $t$ subscript, meaning we re-use the same context vector returned by the encoder for every time-step in the decoder.\n",
    "Before, we predicted the next token, $\\hat{y}_{t+1}$, with the linear layer, $f$, only using the top-layer decoder hidden state at that time-step, $s_t$, as $\\hat{y}_{t+1}=f(s_t^L)$. Now, we also pass the embedding of current token, $d(y_t)$ and the context vector, $z$ to the linear layer.\n",
    "\n",
    "$$\\hat{y}_{t+1} = f(d(y_t), s_t, z)$$\n",
    "\n",
    "Hypothetically the decoder hidden states, $s_t$, no longer need to contain information about the source sequence as it is always available as an input. Thus, it only needs to contain information about what tokens it has generated so far. The addition of $y_t$ to the linear layer also means this layer can directly see what the token is, without having to get this information from the hidden state. This is just a hypothesis, who knows what the model is actually doing.  Nevertheless, it is a solid intuition and the results seem to indicate that this modifications are a good idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16721,
     "status": "ok",
     "timestamp": 1592439997095,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "QArt4i4xpNpQ"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, emb_dim, hid_dim, dropout_p):\n",
    "    super().__init__()\n",
    "    self.output_dim = output_dim\n",
    "    \n",
    "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "    self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "    self.fc_out = nn.Linear(emb_dim + 2*hid_dim, output_dim)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, input, hidden, context):\n",
    "    # input shp: [batch_sz]\n",
    "    # hidden shp: [1, batch_sz, hid_dim]\n",
    "    # context shp: [1, batch_sz, hid_dim]\n",
    "\n",
    "    input = input.unsqueeze(0) # [1, batch_sz]\n",
    "    embedded = self.dropout(self.embedding(input)) # [1, batch_sz, emb_dim]\n",
    "    embedded_concat = torch.cat((embedded, context), dim = 2) # [1, batch_sz, hid_dim + emb_dim]\n",
    "\n",
    "    output, hidden = self.rnn(embedded_concat, hidden)\n",
    "    # output shp: [1, batch_sz, hid_dim]\n",
    "    # hidden shp: [1, batch_sz, hid_dim]\n",
    "\n",
    "    output_concat = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)),\n",
    "                              dim = 1) # [batch_sz, emb_dim + 2*hid_dim]\n",
    "    prediction = self.fc_out(output_concat) # [batch_sz, output_dim]\n",
    "    \n",
    "    return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBokDl5CpR1L"
   },
   "source": [
    "## Seq2Seq Model\n",
    "\n",
    "Steps:\n",
    "- outputs tensor is created to hold all predictions, $\\hat{Y}$\n",
    "- source sequence is fed into encoder to receive a context vector.\n",
    "- initial decoder hidden state is set to context vector\n",
    "- get a batch of <sos> tokens as the first input.\n",
    "- in a decoding loop:\n",
    "  - insert input token, previous hidden state & context vector into decoder.\n",
    "  - receive prediction & new hidden state\n",
    "  - teacher force to decide whether to use prediction or ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16823,
     "status": "ok",
     "timestamp": 1592439997206,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "ieKB-n6ouUX6"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "    # src: [src_len, batch_sz]\n",
    "    # trg: [trg_len, batch_sz]\n",
    "    \n",
    "    batch_size = src.shape[1]\n",
    "    trg_len = trg.shape[0]\n",
    "    trg_vocab_sz = self.decoder.output_dim\n",
    "\n",
    "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_sz).to(self.device)\n",
    "\n",
    "    context = self.encoder(src)\n",
    "\n",
    "    hidden = context\n",
    "\n",
    "    input = trg[0,:]\n",
    "\n",
    "    for t in range(1, trg_len):\n",
    "      output, hidden = self.decoder(input, hidden, context)\n",
    "\n",
    "      outputs[t] = output\n",
    "\n",
    "      teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "      top1 = output.argmax(1)\n",
    "\n",
    "      input = trg[t] if teacher_force else top1\n",
    "\n",
    "    return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6hQm0TrwlGd"
   },
   "source": [
    "## Training the Seq2Seq Model\n",
    "\n",
    "The training part is mostly the same as tut-1. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1592455904371,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "3ukL9-6LwkMp",
    "outputId": "08fe9a41-e61d-4261-dd7d-a9e3600bb580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 14,220,037 trainable parameters.\n",
      "Epoch: 01 | Time: 0m 41s\n",
      "\tTrain Loss: 5.058 | Train PPL: 157.286\n",
      "\t Val. Loss: 4.905 |  Val. PPL: 134.951\n",
      "Epoch: 02 | Time: 0m 42s\n",
      "\tTrain Loss: 4.255 | Train PPL:  70.445\n",
      "\t Val. Loss: 4.767 |  Val. PPL: 117.549\n",
      "Epoch: 03 | Time: 0m 42s\n",
      "\tTrain Loss: 3.845 | Train PPL:  46.742\n",
      "\t Val. Loss: 4.393 |  Val. PPL:  80.844\n",
      "Epoch: 04 | Time: 1m 2s\n",
      "\tTrain Loss: 3.456 | Train PPL:  31.690\n",
      "\t Val. Loss: 4.002 |  Val. PPL:  54.731\n",
      "Epoch: 05 | Time: 1m 16s\n",
      "\tTrain Loss: 3.092 | Train PPL:  22.013\n",
      "\t Val. Loss: 3.841 |  Val. PPL:  46.583\n",
      "Epoch: 06 | Time: 1m 15s\n",
      "\tTrain Loss: 2.803 | Train PPL:  16.486\n",
      "\t Val. Loss: 3.749 |  Val. PPL:  42.475\n",
      "Epoch: 07 | Time: 1m 15s\n",
      "\tTrain Loss: 2.549 | Train PPL:  12.788\n",
      "\t Val. Loss: 3.586 |  Val. PPL:  36.092\n",
      "Epoch: 08 | Time: 1m 19s\n",
      "\tTrain Loss: 2.347 | Train PPL:  10.459\n",
      "\t Val. Loss: 3.576 |  Val. PPL:  35.719\n",
      "Epoch: 09 | Time: 1m 18s\n",
      "\tTrain Loss: 2.144 | Train PPL:   8.534\n",
      "\t Val. Loss: 3.565 |  Val. PPL:  35.343\n",
      "Epoch: 10 | Time: 1m 17s\n",
      "\tTrain Loss: 1.976 | Train PPL:   7.216\n",
      "\t Val. Loss: 3.597 |  Val. PPL:  36.473\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT_P = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, DROPOUT_P)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DROPOUT_P)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "  for name, param in m.named_parameters():\n",
    "    nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# define a function that will calculate the number of trainable parameters in the model.\n",
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters.\")\n",
    "\n",
    "# function to tell us how long an epoch takes.\n",
    "def epoch_time(start_time, end_time):\n",
    "  elapsed_time = end_time - start_time\n",
    "  elapsed_mins = int(elapsed_time / 60)\n",
    "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "  return elapsed_mins, elapsed_secs \n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# The CrossEntropyLoss function calculates both the log softmax as well \n",
    "# as the negative log-likelihood of our predictions.\n",
    "# Our loss function calculates the average loss per token, however by passing\n",
    "# the index of the <pad> token as the ignore_index argument we ignore the loss \n",
    "# whenever the target token is a padding token.\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "# Training Loop\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  for i, batch in enumerate(iterator):\n",
    "    src = batch.src\n",
    "    trg = batch.trg # shp: [trg_len, batch_sz]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(src, trg) # shp: [trg_len, batch_sz, output_dim]\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output[1:].view(-1, output_dim) # shp: [(trg_len-1)*batch_sz, output_dim]\n",
    "    trg = trg[1:].view(-1) # shp: [(trg_len-1)*batch_sz]\n",
    "\n",
    "    loss = criterion(output, trg)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss / len(iterator)\n",
    "\n",
    "# Evaluation Loop\n",
    "def evaluate(model, iterator, criterion):\n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(iterator):\n",
    "      src = batch.src\n",
    "      trg = batch.trg # shp: [trg_len, batch_sz]\n",
    "\n",
    "      output = model(src, trg, teacher_forcing_ratio=0.0)  # shp: [trg_len, batch_sz, output_dim]\n",
    "\n",
    "      output_dim = output.shape[-1]\n",
    "\n",
    "      output = output[1:].view(-1, output_dim)\n",
    "      trg = trg[1:].view(-1)\n",
    "\n",
    "      loss = criterion(output, trg)\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  start_time = time.time()\n",
    "\n",
    "  train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "  valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "  if valid_loss < best_valid_loss:\n",
    "    best_valid_loss = best_valid_loss\n",
    "    torch.save(model.state_dict(), sandbox_path + 'tut2-model.pt')\n",
    "\n",
    "  print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\") \n",
    "  print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "  print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9720,
     "status": "ok",
     "timestamp": 1592456295586,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "je7xJg0tzTvj",
    "outputId": "17fcc05a-09c1-408c-b46a-23db7dbadbaa"
   },
   "outputs": [],
   "source": [
    "# Load the best model with best validation and run the test set\n",
    "model.load_state_dict(torch.load(sandbox_path + 'tut2-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP51JzJ1/NeHYHmAm541wgl",
   "collapsed_sections": [],
   "name": "NLP-bentrevett-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
