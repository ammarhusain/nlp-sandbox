{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7o4Lz35bhRTG"
   },
   "source": [
    "##6 - Attention is All You Need\n",
    "[Paper](https://arxiv.org/abs/1706.03762) [[Tutorial](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb)]\n",
    "\n",
    "This is the basic and vanilla Transformer paper.\n",
    "\n",
    "Differences between this notebook and the paper:\n",
    "- Learned positional encoding compared with a static one.\n",
    "- Standard Adam optimizer with static learning rate.\n",
    "- No label smoothing.\n",
    "\n",
    "These changes closely follow BERT's setup & a majority of other Transformer variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ly59IEDdgw_q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.6.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from torchtext==0.6.0) (0.1.91)\n",
      "Requirement already satisfied: tqdm in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from torchtext==0.6.0) (4.41.1)\n",
      "Requirement already satisfied: torch in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from torchtext==0.6.0) (1.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from torchtext==0.6.0) (1.18.1)\n",
      "Requirement already satisfied: requests in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from torchtext==0.6.0) (2.24.0)\n",
      "Requirement already satisfied: six in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from torchtext==0.6.0) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from torch->torchtext==0.6.0) (0.18.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (2020.6.20)\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: de_core_news_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz#egg=de_core_news_sm==2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from de_core_news_sm==2.3.0) (2.3.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.9.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.18.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.41.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.25.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/de_core_news_sm\n",
      "-->\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n",
      "<spacy.lang.en.English object at 0x7f7771244590>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Hello. how do you do good, sir!\n",
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
      "Unique token in SRC (de) vocab: 7854\n",
      "Unique token in SRC (en) vocab: 5893\n"
     ]
    }
   ],
   "source": [
    "sandbox_path = \"models/\"\n",
    "\n",
    "# Install a newer version of troch text than what is default\n",
    "!pip install torchtext==0.6.0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_de = spacy.load('de')\n",
    "print(spacy_en)\n",
    "\n",
    "def tokenize_de(text):\n",
    "  \"Tokenize a German language string\"\n",
    "  # Not reversing for this one\n",
    "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "  return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "foo = spacy_en.tokenizer(\"Hello. how do you do good, sir!\")\n",
    "print(type(foo))\n",
    "print(foo)\n",
    "[type(t.text) for t in foo]\n",
    "\n",
    "SRC = Field(tokenize = tokenize_de, init_token='<sos>', \n",
    "            eos_token='<eos>', lower=True, batch_first = True)\n",
    "TRG = Field(tokenize = tokenize_en, init_token='<sos>', \n",
    "            eos_token='<eos>', lower=True, batch_first = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "print(f\"Unique token in SRC (de) vocab: {len(SRC.vocab)}\")\n",
    "print(f\"Unique token in SRC (en) vocab: {len(TRG.vocab)}\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LoK5KWP2fIR4"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "Similar to the ConvSeq2Seq model, the Transformer's encoder does not attempt to compress the entire source sentence. Instead it produces a sequence of context vectors, one for each input source token. They are called context vectors instead of hidden states because a hidden state at time `t` has only seen tokens preceding it, while a context vector takes into account all source tokens.\n",
    "\n",
    "The source mask, `src_mask`, is simply the same shape as the source sentence but has a value of 1 when the token in the source sentence is not a <pad> token and 0 when it is a <pad> token. This is used in the encoder layers to mask the multi-headed attention mechanisms, which are used to calculate and apply attention over the source sentence so the model does not pay attention to <pad> tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gDpppcCkey2"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout_p,\n",
    "               device, max_length = 100):\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = device\n",
    "\n",
    "    self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "    self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "    self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, \n",
    "                                              dropout_p, device)\n",
    "                                              for _ in range(n_layers)])\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "  def forward(self, src, src_mask):\n",
    "    # src: [batch_sz, src_len]\n",
    "    # src_mask: [batch_sz, src_len]\n",
    "    batch_sz = src.shape[0]\n",
    "    src_len = src.shape[1]\n",
    "\n",
    "    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_sz, 1).to(self.device) # [batch_sz, src_len]\n",
    "\n",
    "    src = self.dropout((self.tok_embedding(src) * self.scale)\n",
    "                           + self.pos_embedding(pos))  # [batch_sz, src_len, hid_dim]\n",
    "\n",
    "    for layer in self.layers:\n",
    "      src = layer(src, src_mask) # [batch_sz, src_len, hid_dim]\n",
    "\n",
    "    return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnTOiodcnHqN"
   },
   "source": [
    "### Encoder Layer\n",
    "\n",
    "This is where all of the \"meat\" is. \n",
    "- Pass the source sentence and its mask into the multi-headed attention layer\n",
    "- Perform dropout\n",
    "- Apply a residual connection\n",
    "- Pass it through a Layer Normalization layer\n",
    "- Pass it through a position-wide feedforward layer\n",
    "- Apply dropout\n",
    "- Residual connection\n",
    "- Layer Normalization\n",
    "This is the output of the layer that is then fed into the next layer as input. Parameters are not shared between layers.\n",
    "\n",
    "Gist of layer normalization is that it normalizes the values of the features, ie across each hidden dimension to a mean of 0 and std dev of 1. This allows large neural networks to be trained easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-l__yG0uK0P"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, hid_dim, n_heads, pf_dim,\n",
    "               dropout_p, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "    self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "    self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, \n",
    "                                                  dropout_p, device)\n",
    "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim,\n",
    "                                                                 dropout_p)\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, src, src_mask):\n",
    "    # src: [batch_sz, src_len, hid_dim]\n",
    "    # src_mask: [batch_sz, src_len]\n",
    "\n",
    "    # Compute self-attention\n",
    "    _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "\n",
    "    src = self.self_attn_layer_norm(src + self.dropout(_src)) # [batch_sz, src_len, hid_dim]\n",
    "\n",
    "    # Positionwise feedforward\n",
    "    _src = self.positionwise_feedforward(src) # dim?\n",
    "\n",
    "    src = self.ff_layer_norm(src + self.dropout(_src)) # [batch_sz, src_len, hid_dim]\n",
    "\n",
    "    return src\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gFI_FpXv_MO"
   },
   "source": [
    "### Multi Head Attention Layer\n",
    "\n",
    "This is one of the key, novel concepts introduced by the Transformer paper.\n",
    "\n",
    "Attention can be thought of as queries, keys and values - where the query is used with the key to get an attention vector (usually output of a softmax operation where all values are between 0 & 1 and sum to 1), which is then used to get a weighted sum of values.\n",
    "\n",
    "Transformer uses scaled dot-product attention:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$\n",
    "\n",
    "This is similar to standard dot product attention but is scaled by $d_k$ , which according to the paper is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
    "\n",
    "Instead of doing a single attention application the queries, keys & values have their hid_dim split into $h$ heads and the scaled dot-product attention is calculated over all head in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$.\n",
    "\n",
    "Steps:\n",
    "- Project Q, K & V through a linear layer each `fc_q`, `fc_k`, `fc_v`. These layers map hid_dim to hid_dim. \n",
    "- Split hid_dim into n_heads using `.view()`. Each head gets `head_dim = hid_dim // n_heads` sized query, key & value vectors.\n",
    "- Calculate the energy (unnormalized attention) by multiplyign Q & K and scaling it by square root of head_dim. \n",
    "- Mask the energy over the pad tokens, apply softmax and dropout.\n",
    "- Apply the attention to the value vectors.\n",
    "- Combine them back together (using `view()`) to create a `hid_dim` size vector.\n",
    "- Project through `fc_o` linear layer that maps hid_dim to hid_dim.\n",
    "\n",
    "One thing strange is that dropout is applied straight to the attention. This means that the attention vector will most likely not sum to 1 and we may want to pay full attention to a token but the attention over that token may get set 0 by dropout. This is never explained but is used in almost all Transformer variants (including BERT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_OxwT-HAAMH"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "  def __init__(self, hid_dim, n_heads, dropout_p, device):\n",
    "    super().__init__()\n",
    "\n",
    "    assert hid_dim % n_heads == 0\n",
    "\n",
    "    self.hid_dim = hid_dim\n",
    "    self.n_heads = n_heads\n",
    "    self.head_dim = hid_dim // n_heads\n",
    "\n",
    "    self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "    self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "    self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "    \n",
    "    self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "  def forward(self, query, key, value, mask = None):\n",
    "    batch_sz = query.shape[0]\n",
    "    # query, key, value: [batch_sz, sent_len, hid_dim]\n",
    "\n",
    "    Q = self.fc_q(query) # [batch_sz, q_len, hid_dim]\n",
    "    K = self.fc_k(key) # [batch_sz, k_len, hid_dim]\n",
    "    V = self.fc_v(value) # [batch_sz, v_len, hid_dim]\n",
    "\n",
    "    # Split in to n_heads for multi head attention.\n",
    "    # Permute to get the last 2 dimensions as sent_len, head_dim\n",
    "    Q = Q.view(batch_sz, -1, self.n_heads, self.head_dim)\\\n",
    "                                          .permute(0,2,1,3) # [batch_sz, n_heads, q_len, head_dim] \n",
    "    K = K.view(batch_sz, -1, self.n_heads, self.head_dim)\\\n",
    "                                          .permute(0,2,1,3) # [batch_sz, n_heads, k_len, head_dim] \n",
    "    V = V.view(batch_sz, -1, self.n_heads, self.head_dim)\\\n",
    "                                          .permute(0,2,1,3) # [batch_sz, n_heads, v_len, head_dim] \n",
    "\n",
    "    energy = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale  # [batch_sz, n_heads, q_len, k_len]\n",
    "\n",
    "    # AH: Figure out what the mask dim is? how does it work?\n",
    "    if mask is not None:\n",
    "      energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "    attention = torch.softmax(energy, dim = -1) # [batch_sz, n_heads, q_len, k_len]\n",
    "\n",
    "    # This works because k_len and v_len are guaranteed to be the same.\n",
    "    # because either Q,K,V all come from the src or trg (for self attention head) or \n",
    "    # Q comes from trg and K & V come from src (second attention layer of decoder)\n",
    "    x = torch.matmul(self.dropout(attention), # weird place to apply dropout \n",
    "                     V) # [batch_sz, n_heads, q_len, head_dim]\n",
    "\n",
    "    # join the heads back together to get hid_dim\n",
    "    x = x.permute(0,2,1,3).contiguous() # [batch_sz, q_len, n_heads, head_dim]\n",
    "    x = x.view(batch_sz, -1, self.hid_dim) # [batch_sz, q_len, hid_dim]\n",
    "\n",
    "    x = self.fc_o(x) # [batch_sz, q_len, hid_dim]\n",
    "\n",
    "    return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCJu9QuhpvNW"
   },
   "source": [
    "### Position-wise Feedforward Layer\n",
    "\n",
    "This is a simple block. It simply takes the output of multi-head attention with size `hid_dim` (512) projects it to a much higher dimension of `pf_dim` (2048), applies ReLU and then projects it back down to `hid_dim` (512).\n",
    "\n",
    "It is unclear/unexplained why this layer is needed. BERT uses the GELU activation instead of ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lEjl17bqheO"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "  def __init__(self, hid_dim, pf_dim, dropout_p):\n",
    "    super().__init__()\n",
    "\n",
    "    self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "    self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x: [batch_sz, sent_len, hid_dim]\n",
    "\n",
    "    x = self.dropout(torch.relu(self.fc_1(x))) # [batch_sz, sent_len, pf_dim]\n",
    "    x = self.fc_2(x) # [batch_sz, sent_len, hid_dim]\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKcyBIhorVVW"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is similar to encoder, however it now has an additional multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value.\n",
    "\n",
    "The decoder representation after the Nth layer is passed through a linear layer `fc_out`. In PyTorch the softmax operation is contained within our loss function, so we do not explicitly pass it through a softmax at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8y7l_w4biBd"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim,\n",
    "               dropout_p, device, max_length = 100):\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = device\n",
    "\n",
    "    self.tok_emedding = nn.Embedding(output_dim, hid_dim)\n",
    "    self.pos_emedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "    self.layers = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim,\n",
    "                                              dropout_p, device)\n",
    "                                for _ in range(n_layers)])\n",
    "    self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "    # trg: [batch_sz, trg_len]\n",
    "    # enc_src: [batch_sz, src_len, hid_dim]\n",
    "    # trg_mask: [batch_sz, trg_len]\n",
    "    # src_mask: [batch_sz, src_len]\n",
    "\n",
    "    batch_sz = trg.shape[0]\n",
    "    trg_len = trg.shape[1]\n",
    "\n",
    "    pos = torch.arange(0, trg_len).unsqueeze(0).\\\n",
    "                    repeat(batch_sz, 1).to(self.device) # [batch_sz, trg_len]\n",
    "\n",
    "    trg = self.dropout((self.tok_embedding(trg) * self.scale) + \n",
    "                        self.pos_embedding(pos)) # [batch_sz, trg_len, hid_dim]\n",
    "    \n",
    "    for layer in self.layers:\n",
    "      trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "      # trg: [batch_sz, trg_len, hid_dim]\n",
    "      # attention: [batch_sz, n_heads, trg_len, src_len]\n",
    "\n",
    "    output = self.fc_out(trg) # [batch_sz, trg_len, output_dim]\n",
    "  \n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgVnSZcOfDeq"
   },
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oTyOViHfF6h"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self, hid_dim, n_heads, pf_dim, dropout_p, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "    self.src_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "    self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "\n",
    "    self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, \n",
    "                                                  dropout_p, device)\n",
    "    self.src_attention = MultiHeadAttentionLayer(hid_dim, n_heads, \n",
    "                                                  dropout_p, device)\n",
    "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim,\n",
    "                                                                 dropout_p)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "    # trg: [batch_sz, trg_len, hid_dim]\n",
    "    # enc_src: [batch_sz, src_len, hid_dim]\n",
    "    # trg_mask: [batch_sz, trg_len]\n",
    "    # src_mask: [batch_sz, src_len]\n",
    "\n",
    "    # self attention\n",
    "    _trg, _ = self.self_attention(trg, trg, trg, trg_mask) # [batch_sz, trg_len, hid_dim]\n",
    "\n",
    "    # dropout, residual connection and layer norm\n",
    "    trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "    # attention over the src sentence\n",
    "    _trg, attention = self.src_attention(trg, enc_src, enc_src, trg_mask) \n",
    "    # _trg: [batch_sz, trg_len, hid_dim]\n",
    "    # attention: [batch_sz, n_heads, trg_len, src_len]\n",
    "\n",
    "    trg = self.src_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "    # positionwise feedforward\n",
    "    _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "    trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "    return trg, attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCbUI6ydkPRN"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNjaRARH8vS6J9ifbo8W6XB",
   "collapsed_sections": [],
   "name": "NLP-bentrevett-6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
