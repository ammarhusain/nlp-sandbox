{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FctKaL252S7l"
   },
   "source": [
    "# 3 - Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "In this third notebook on sequence-to-sequence models using PyTorch and TorchText, we'll be implementing the model from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). This model achives our best perplexity yet, ~27 compared to ~34 for the previous model.\n",
    "\n",
    "In the last tut-2 we tries to alleviate information compression by passing the context vector(last hidden state) of encoder at every decoder step. Though the context vector still needs to summarize well. This time, we'll use Attention!.\n",
    "\n",
    "Attention works by first, calculating an attention vector $a$, that is the length of the source sentence. The attention vector has the property that each element is between 0 and 1, and the entire vector sums to 1. We then calculate a weighted sum of our source sentence hidden states $H$, to get a weighted source vector, $w$.\n",
    "$$w = \\sum_{i}a_ih_i$$\n",
    "We calculate a new weighted source vector every time-step when decoding, using it as input to our decoder RNN as well as the linear layer to make a prediction.\n",
    "\n",
    "## Preparing Data\n",
    "\n",
    "Preparation is same as previous tut-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68512,
     "status": "ok",
     "timestamp": 1592494881865,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "zU90ZJfF2Rux",
    "outputId": "b7f3524c-176a-4e87-9da4-e2bfb40fad0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: de_core_news_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz#egg=de_core_news_sm==2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from de_core_news_sm==2.3.0) (2.3.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.9.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.6.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.41.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/de_core_news_sm\n",
      "-->\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n",
      "<spacy.lang.en.English object at 0x7f9d47666c90>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Hello. how do you do good, sir!\n",
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
      "Unique token in SRC (de) vocab: 7854\n",
      "Unique token in SRC (en) vocab: 5893\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "sandbox_path = \"models/\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_de = spacy.load('de')\n",
    "print(spacy_en)\n",
    "\n",
    "def tokenize_de(text):\n",
    "  \"Tokenize a German language string\"\n",
    "  # Not reversing for this one\n",
    "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "  return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "foo = spacy_en.tokenizer(\"Hello. how do you do good, sir!\")\n",
    "print(type(foo))\n",
    "print(foo)\n",
    "[type(t.text) for t in foo]\n",
    "\n",
    "SRC = Field(tokenize = tokenize_de, init_token='<sos>', \n",
    "            eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize = tokenize_en, init_token='<sos>', \n",
    "            eos_token='<eos>', lower=True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "print(f\"Unique token in SRC (de) vocab: {len(SRC.vocab)}\")\n",
    "print(f\"Unique token in SRC (en) vocab: {len(TRG.vocab)}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8rhxVtpJl-2"
   },
   "source": [
    "## Encoder\n",
    "We use a single layer GRU, however now it is bidirectional. With a bidirectional RNN, we have two RNNs in each layer. A forward RNN going over the embedded sentence from left to right, and a backward RNN going over the embedded sentence from right to left. We'll also get two context vectors, one from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$, and one from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$. \n",
    "\n",
    "outputs is of size [src len, batch size, hid dim * num directions] where the first hid_dim elements in the third axis are the hidden states from the top layer forward RNN, and the last hid_dim elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together\n",
    "\n",
    "hidden is of size [n layers * num directions, batch size, hid dim], where [-2, :, :] gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and [-1, :, :] gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n",
    "\n",
    "As the decoder is not bidirectional, it only needs a single context vector $z$ to use as its hidden state $s_0$ and we currently have two (a forward & a backward). We solve this by concatenating the 2 context vectors, passing them through a linear layer and applying a tanh actiovation. **Note:** Paper only uses the first backward RNN hidden state through a linear layer to get decoder initial hidden state.\n",
    "\n",
    "As we want our model to look back over the whole of the source sentence we return outputs, aka the stacked forward & backward hidden states for every token in the source sentence. We also return hidden, which acts as our initial hidden state in the decoder.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68494,
     "status": "ok",
     "timestamp": 1592494881867,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "tlhR8zS9Mj5o"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout_p):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "    self.fc = nn.Linear(2 * enc_hid_dim, dec_hid_dim)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, src):\n",
    "    # src: [src_len, batch_sz]\n",
    "\n",
    "    embedded = self.dropout(self.embedding(src)) # [src_len, batch_sz, emb_dim]\n",
    "    outputs, enc_hidden = self.rnn(embedded)\n",
    "    # outputs: [src_len, batch_sz, num_directions*hid_dim]\n",
    "    # enc_hidden: [num_directions, batch_sz, hid_dim]\n",
    "\n",
    "    # project the final hidden state to get an initial decoder state.\n",
    "    proj_hidden = torch.tanh(self.fc(\n",
    "        torch.cat((enc_hidden[0,:,:], enc_hidden[1,:,:]), dim=1))) # [batch_sz, dec_hid_dim]\n",
    "\n",
    "    return outputs, proj_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqJ9KfqPQwYG"
   },
   "source": [
    "## Attention\n",
    "This will take in the previous hidden state of the decoder $s_{t-1}$ and all of the stacked forward & backward hidden states from the encoder $H$. The layer will output an attention vector $a_t$ that is the length of the source sentence, with each element between 0 & 1 and the entire vector sums to 1.\n",
    "\n",
    "We implement **Additive Attention** (as described in XCS224N) for this paper. HWs were basic dot product attention. \n",
    "First, we calculate the energy between the previous decoder hidden state and the encoder hidden states. As our encoder hidden states are a sequence of T tensors and our previous decoder hidden state is a single tensor, the first thing we do is repeat the previous decoder hidden state T times. We then calculate the energy $E_t$ between them by concatenating them together and passing them through a linear layer (`attn`) and tanh activation. This can be thought of as calculating how well each encoder hidden state \"matches\" the previous hidden state. \n",
    "\n",
    "We then take a time invariant tensor $v$ and compute a dot product with it giving us a [1, src_len] sized tensor. We implement $v$ as a linear layer without a bias. This is done because in theory we can use the `attn` layer to project the concatenated hidden states into some arbitrary dimension `d3` and then dot a vector `v` of size `d3` to get a score. Here we just use `d3` as `dec_hid_dim`. See lecture slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68458,
     "status": "ok",
     "timestamp": 1592494881869,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "MwPqiw14UkTu"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.attn = nn.Linear(2*enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
    "    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs):\n",
    "    # hidden: [batch_sz, dec_hid_dim]\n",
    "    # encoder_outputs: [src_len, batch_sz, 2*enc_hid_dim]\n",
    "\n",
    "    batch_sz = encoder_outputs.shape[1]\n",
    "    src_len = encoder_outputs.shape[0]\n",
    "\n",
    "    # repeat decoder hidden state for src_len times (tile it)\n",
    "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1) # [batch_sz, src_len, dec_hid_dim]\n",
    "\n",
    "    encoder_outputs = encoder_outputs.permute(1, 0, 2) # [batch_sz, src_len, 2*hid_dim]\n",
    "\n",
    "    energy = torch.tanh(self.attn(\n",
    "        torch.cat((hidden, encoder_outputs), dim = 2))) # [batch_sz, src_len, dec_hid_dim]\n",
    "\n",
    "    attention = self.v(energy).squeeze(2) # [batch_sz, src_len]\n",
    "\n",
    "    return F.softmax(attention, dim = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3Exj2BMbX6u"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder contains the attention layer, which takes the previous hidden state and all of the encoder hidden states and returns an attention vector, $a_t$.\n",
    "\n",
    "We then use the attention vector to create a weighted source vector (weighted sum of the encoder hidden states) $w_t$ using $a_t$ as the weights.\n",
    "\n",
    "We then concatenate the embedded input word and the weighted source vector $w_t$ and pass it to a GRU with the previous hidden state $s_{t-1}$. \n",
    "\n",
    "Finally we concatenate the embedded input word, the weighted source vector $w_t$ and the current hidden state $s_t$ and pass it through a linear layer to make a next target word prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68445,
     "status": "ok",
     "timestamp": 1592494881870,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "z-tnTaZriNH0"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, emb_dim, enc_hid_dim,\n",
    "               dec_hid_dim, dropout_p, attention):\n",
    "    super().__init__()\n",
    "    self.output_dim = output_dim\n",
    "    self.attention = attention\n",
    "    \n",
    "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "    self.rnn = nn.GRU(2*enc_hid_dim + emb_dim, dec_hid_dim)\n",
    "    self.fc_out = nn.Linear(2*enc_hid_dim + emb_dim + dec_hid_dim, output_dim)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, input, hidden, encoder_outputs):\n",
    "    # input: [batch_sz]\n",
    "    # hidden: [batch_sz, dec_hid_dim]\n",
    "    # encoder_outputs: [src_len, batch_sz, 2*enc_hid_dim]\n",
    "\n",
    "    input = input.unsqueeze(0) # [1, batch_sz]\n",
    "    embedded = self.dropout(self.embedding(input)) # [1, batch_sz, emb_dim]\n",
    "\n",
    "    a = self.attention(hidden, encoder_outputs) # [batch_sz, src_len]\n",
    "\n",
    "    a = a.unsqueeze(1) # [batch_sz, 1, src_len]\n",
    "\n",
    "    encoder_outputs = encoder_outputs.permute(1,0,2) # [batch_sz, src_len, 2*enc_hid_dim]\n",
    "\n",
    "    weighted = torch.bmm(a, encoder_outputs) # [batch_sz, 1, 2*enc_hid_dim]\n",
    "\n",
    "    weighted = weighted.permute(1, 0, 2) # [1, batch_sz, 2*enc_hid_dim]\n",
    "\n",
    "    rnn_input = torch.cat((embedded, weighted), dim = 2) # [1, batch_sz, 2*enc_hid_dim + emb_dim]\n",
    "\n",
    "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "    # output: [1, batch_sz, dec_hid_dim]\n",
    "    # hidden: [1, batch_sz, dec_hid_dim]\n",
    "    # in this case output == hidden\n",
    "    assert (output == hidden).all()\n",
    "\n",
    "    embedded = embedded.squeeze(0)\n",
    "    output = output.squeeze(0)\n",
    "    weighted = weighted.squeeze(0)\n",
    "    prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) # [batch_sz, output_dim]\n",
    "\n",
    "    return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGo3cSiTpY7-"
   },
   "source": [
    "## Seq2Seq Model\n",
    "\n",
    "This Seq2Seq is similar to the previous two. The only addition is that the encoder returns all its hidden states. These are received and passed forward to the decoder for it to compute attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68436,
     "status": "ok",
     "timestamp": 1592494881871,
     "user": {
      "displayName": "Ammar Husain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64",
      "userId": "10057289260862428677"
     },
     "user_tz": 420
    },
    "id": "kwI9c5Qtpl-u"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "    # src: [src_len, batch_sz]\n",
    "    # trg: [trg_len, batch_sz]\n",
    "\n",
    "    batch_size = src.shape[1]\n",
    "    trg_len = trg.shape[0]\n",
    "    trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "    # tensor to store decoder outputs\n",
    "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "    encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "    input = trg[0,:]\n",
    "\n",
    "    for t in range(1, trg_len):\n",
    "      output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "\n",
    "      outputs[t] = output\n",
    "\n",
    "      teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "      top1 = output.argmax(1)\n",
    "\n",
    "      input = trg[t] if teacher_force else top1\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkTksWeBrWtU"
   },
   "source": [
    "## Training Code\n",
    "\n",
    "The rest is very similar to the training code from the previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "aiX83zVcrdcu",
    "outputId": "5cea139a-e2ae-4623-b088-be77185dc737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 20,518,661 trainable parameters.\n",
      "Epoch: 01 | Time: 1m 18s\n",
      "\tTrain Loss: 4.917 | Train PPL: 136.631\n",
      "\t Val. Loss: 4.709 |  Val. PPL: 110.896\n",
      "Epoch: 02 | Time: 1m 28s\n",
      "\tTrain Loss: 3.968 | Train PPL:  52.878\n",
      "\t Val. Loss: 4.159 |  Val. PPL:  64.039\n",
      "Epoch: 03 | Time: 2m 15s\n",
      "\tTrain Loss: 3.290 | Train PPL:  26.842\n",
      "\t Val. Loss: 3.624 |  Val. PPL:  37.484\n",
      "Epoch: 04 | Time: 2m 15s\n",
      "\tTrain Loss: 2.785 | Train PPL:  16.192\n",
      "\t Val. Loss: 3.369 |  Val. PPL:  29.061\n",
      "Epoch: 05 | Time: 1m 44s\n",
      "\tTrain Loss: 2.445 | Train PPL:  11.534\n",
      "\t Val. Loss: 3.251 |  Val. PPL:  25.818\n",
      "Epoch: 06 | Time: 1m 23s\n",
      "\tTrain Loss: 2.145 | Train PPL:   8.543\n",
      "\t Val. Loss: 3.224 |  Val. PPL:  25.119\n",
      "Epoch: 07 | Time: 1m 18s\n",
      "\tTrain Loss: 1.915 | Train PPL:   6.784\n",
      "\t Val. Loss: 3.286 |  Val. PPL:  26.727\n",
      "Epoch: 08 | Time: 1m 18s\n",
      "\tTrain Loss: 1.716 | Train PPL:   5.561\n",
      "\t Val. Loss: 3.234 |  Val. PPL:  25.378\n",
      "Epoch: 09 | Time: 1m 18s\n",
      "\tTrain Loss: 1.577 | Train PPL:   4.841\n",
      "\t Val. Loss: 3.272 |  Val. PPL:  26.376\n",
      "Epoch: 10 | Time: 2m 26s\n",
      "\tTrain Loss: 1.443 | Train PPL:   4.235\n",
      "\t Val. Loss: 3.259 |  Val. PPL:  26.034\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT_P = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT_P)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT_P, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "  for name, param in m.named_parameters():\n",
    "    nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# define a function that will calculate the number of trainable parameters in the model.\n",
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters.\")\n",
    "\n",
    "# function to tell us how long an epoch takes.\n",
    "def epoch_time(start_time, end_time):\n",
    "  elapsed_time = end_time - start_time\n",
    "  elapsed_mins = int(elapsed_time / 60)\n",
    "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "  return elapsed_mins, elapsed_secs \n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# The CrossEntropyLoss function calculates both the log softmax as well \n",
    "# as the negative log-likelihood of our predictions.\n",
    "# Our loss function calculates the average loss per token, however by passing\n",
    "# the index of the <pad> token as the ignore_index argument we ignore the loss \n",
    "# whenever the target token is a padding token.\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "# Training Loop\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  for i, batch in enumerate(iterator):\n",
    "    src = batch.src\n",
    "    trg = batch.trg # shp: [trg_len, batch_sz]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(src, trg) # shp: [trg_len, batch_sz, output_dim]\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output[1:].view(-1, output_dim) # shp: [(trg_len-1)*batch_sz, output_dim]\n",
    "    trg = trg[1:].view(-1) # shp: [(trg_len-1)*batch_sz]\n",
    "\n",
    "    loss = criterion(output, trg)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss / len(iterator)\n",
    "\n",
    "# Evaluation Loop\n",
    "def evaluate(model, iterator, criterion):\n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(iterator):\n",
    "      src = batch.src\n",
    "      trg = batch.trg # shp: [trg_len, batch_sz]\n",
    "\n",
    "      output = model(src, trg, teacher_forcing_ratio=0.0)  # shp: [trg_len, batch_sz, output_dim]\n",
    "\n",
    "      output_dim = output.shape[-1]\n",
    "\n",
    "      output = output[1:].view(-1, output_dim)\n",
    "      trg = trg[1:].view(-1)\n",
    "\n",
    "      loss = criterion(output, trg)\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  start_time = time.time()\n",
    "\n",
    "  train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "  valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "  if valid_loss < best_valid_loss:\n",
    "    best_valid_loss = best_valid_loss\n",
    "    torch.save(model.state_dict(), sandbox_path + 'tut3-model.pt')\n",
    "\n",
    "  print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\") \n",
    "  print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "  print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXWdk1mssMsc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.286 | Test PPL:  26.744 |\n"
     ]
    }
   ],
   "source": [
    "# Load the best model with best validation and run the test set\n",
    "model.load_state_dict(torch.load(sandbox_path + 'tut3-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPL31RdSLmey5my5ZIANynK",
   "collapsed_sections": [],
   "name": "NLP-bentrevett-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
