{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0unEC1gkk0D"
   },
   "source": [
    "# 5 - Convolutional Sequence to Sequence Learning\n",
    "In this notebook we'll be implementing the [Convolutional Sequence to Sequence Learning model](https://arxiv.org/abs/1705.03122). [[Tutorial](https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb)]\n",
    "\n",
    "This model is drastically different than the previous models, in that it does not\n",
    "use recurrent networks. It will instead use convolutional layers. A convolutional layer uses filters. \n",
    "These filters have a width and a height in case of images). If a filter has a width of 3, it can see\n",
    "3 consecutive tokens. Each conv layer has many such filters (1024 in this tutorial).\n",
    "Each filter slides from beginning to end, looking at all 3 consecutive tokens at a time. The idea is that each of the 1024 filters will learn to extract a different feature from the text. The result of this feature extraction will then be used by the model potentially as input to another conv layer. This can then all be used to extract features from the source sentence to translate it into the target language.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Mostly the same as previous tutorials. The only difference is that we now set `batch_first` to `True` to get SRC & TRG tokens as [batch_size, seq_len]. Since the previous tutorials used RNNs, PyTorch RNN models required the tensor to be of shape [seq_len, batch_size]. However now we use conv models so batch_size needs to be first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfpxR9z8iB_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.6.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from torchtext==0.6.0) (0.1.91)\n",
      "Requirement already satisfied: torch in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from torchtext==0.6.0) (1.5.1)\n",
      "Requirement already satisfied: six in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from torchtext==0.6.0) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from torchtext==0.6.0) (1.18.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from torchtext==0.6.0) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from torchtext==0.6.0) (2.24.0)\n",
      "Requirement already satisfied: future in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from torch->torchtext==0.6.0) (0.18.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.6.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: de_core_news_sm==2.3.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz#egg=de_core_news_sm==2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from de_core_news_sm==2.3.0) (2.3.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.18.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/google/home/ammarh/.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.41.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.6.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.9.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/de_core_news_sm\n",
      "-->\n",
      "/usr/local/google/home/ammarh/anaconda3/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n",
      "<spacy.lang.en.English object at 0x7fc0c2079bd0>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Hello. how do you do good, sir!\n",
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
      "Unique token in SRC (de) vocab: 7854\n",
      "Unique token in SRC (en) vocab: 5893\n"
     ]
    }
   ],
   "source": [
    "sandbox_path = \"models/\"\n",
    "\n",
    "# Install a newer version of troch text than what is default\n",
    "!pip install torchtext==0.6.0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_de = spacy.load('de')\n",
    "print(spacy_en)\n",
    "\n",
    "def tokenize_de(text):\n",
    "  \"Tokenize a German language string\"\n",
    "  # Not reversing for this one\n",
    "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "  return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "foo = spacy_en.tokenizer(\"Hello. how do you do good, sir!\")\n",
    "print(type(foo))\n",
    "print(foo)\n",
    "[type(t.text) for t in foo]\n",
    "\n",
    "SRC = Field(tokenize = tokenize_de, init_token='<sos>', \n",
    "            eos_token='<eos>', lower=True, batch_first = True)\n",
    "TRG = Field(tokenize = tokenize_en, init_token='<sos>', \n",
    "            eos_token='<eos>', lower=True, batch_first = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "print(f\"Unique token in SRC (de) vocab: {len(SRC.vocab)}\")\n",
    "print(f\"Unique token in SRC (en) vocab: {len(TRG.vocab)}\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYqNMrEbmJx1"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Previously for recurrent net encoders we produced one context vector that compressed the entire input sequence into the last hidden state (other than for attention computation). In the convolutional seq2seq model, we will get 2 context vectors per input token: *conved vector* and *combined vector*. The conved vector is the output of the last conv layer after being projected down to emb_size. The combined vector is the sum of the conved vector and the original word embedding.\n",
    "\n",
    "First, we pass the token through an embedding layer (which is standard in NLP). However, since there is no recurrence (aka order can be mixed up), we use a second positional embedding layer. The token and positional embeddings are elementwise summed together to get the final embedding vector (containes information about token and its position in the sequence).\n",
    "\n",
    "This vector is then passed through a bunch of conv blocks (10 in this case) for the \"magic\" to happen. After passing through all the conv blocks, the vector is then fed through another linear layer to transform it back from hidden dim size to embedding dim size. This is our conved vector and we have one per token.\n",
    "\n",
    "### Convolutional Block\n",
    "We pad the input sequence in order to maintain its length. Without padding, length will be $filter\\_size - 1$ shorter than the input sequence entering the conv layer (for odd sized filters).\n",
    "\n",
    "These filters are designed so the output hidden dimension of them is twice the input hidden dimension (in computer vision terminology these hidden dims are called channels). We double the size of the output hidden dim because we use a special activation function called gated linear units (GLU). GLUs have gating mechanisms (similar to LSTM & GRU) that half the size of their input - whereas usually activation functions keep the input dimension the same. The output vector from GLU (which is same size as hid dim now) is them elementwise summed with the pre-conved vector (residual connection) producing the output vector for a single conv block. We stack these convolution blocks and subsequent blocks take the output of the previous block and perform the same steps though they do not share their parameter weights. The output of the last conv block if fed through a linear layer that projects from hid dim to emb dim to get the *conved vector*.\n",
    "\n",
    "Note: \n",
    "- The scale variable is used by the authors to \"ensure that the variance throughout the network does not change dramatically\". The performance of the model seems to vary wildly using different seeds if this is not used.\n",
    "- The positional embedding is initialized to have a \"vocabulary\" of 100. This means it can handle sequences up to 100 elements long, indexed from 0 to 99. This can be increased if used on a dataset with longer sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHzDdHb-1dUq"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, kernel_size, \n",
    "               dropout_p, device, max_length = 100):\n",
    "    super().__init__()  \n",
    "    assert kernel_size % 2 == 1 # Ensure odd kernel size\n",
    "\n",
    "    self.device = device\n",
    "\n",
    "    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "\n",
    "    self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "    self.pos_embedding = nn.Embedding(max_length, emb_dim) # positional embedding are learned as well.\n",
    "\n",
    "    self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "    self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "\n",
    "    self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim,\n",
    "                                          out_channels = 2*hid_dim,\n",
    "                                          kernel_size = kernel_size,\n",
    "                                          padding = (kernel_size - 1)//2)\n",
    "                                for _ in range(n_layers)])\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, src):\n",
    "    # src: [batch_sz, src_len]\n",
    "    batch_sz = src.shape[0]\n",
    "    src_len = src.shape[1]\n",
    "\n",
    "    # create position tensor\n",
    "    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_sz, 1).to(self.device) # [batch_sz, src_len]\n",
    "    # pos = [0, 1, 2, ... , src_len-1]\n",
    "\n",
    "    tok_embedded = self.tok_embedding(src)\n",
    "    pos_embedded = self.pos_embedding(pos)\n",
    "\n",
    "    embedded = self.dropout(tok_embedded + pos_embedded) # [batch_sz, src_len, emb_dim]\n",
    "\n",
    "    # pass through linear layer to project to hid dim\n",
    "    conv_input = self.emb2hid(embedded) # [batch_sz, src_len, hid_dim]\n",
    "\n",
    "    # permute for convolving across the src length\n",
    "    conv_input = conv_input.permute(0, 2, 1) # [batch_sz, hid_dim, src_len]\n",
    "    \n",
    "    # begin convolution blocks ...\n",
    "    for i, conv in enumerate(self.convs):\n",
    "      # pass through conv layer\n",
    "      conved = conv(self.dropout(conv_input)) # [batch_sz, 2 * hid_dim, src_len]\n",
    "      conved = F.glu(conved, dim = 1) # [batch_sz, hid_dim, src_len]\n",
    "      # apply residual connection\n",
    "      conved = (conved + conv_input) * self.scale # [batch_sz, hid_dim, src_len]\n",
    "      conv_input = conved\n",
    "\n",
    "    # permute and project back to emb dim\n",
    "    conved = self.hid2emb(conved.permute(0,2,1)) # [batch_sz, src_len, emb_dim]\n",
    "\n",
    "    combined = (conved + embedded) * self.scale # [batch_sz, src_len, emb_dim]\n",
    "\n",
    "    return conved, combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ph55y2NBgHEZ"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder predicts all tokens within the sentence in parallel. There is no sequential processing, ie no decogin loop. Decoder is similar to encoder with a few changes within the conv blocks & the main decoder model.\n",
    "\n",
    "First the token embeddings do not have a residual connection that connects after the conv blocks to create a \"combined vector\" (as was the case for the encoder).\n",
    "\n",
    "Second, to compute attention the \"conved\" (key) & \"combined\" (value) vectors from the encoder are used within each conv block of the decoder.\n",
    "\n",
    "Lastly the output of the decoder is a linear layer from emb_dim to output_dim. This is used to predict next word.\n",
    "\n",
    "### Decoder Convolutional Block\n",
    "\n",
    "Instead of padding equally on both sides to ensure sentence length stays the same, we only pad at the beginning of the sentence. This is a trick to make sure the decoder cannot cheat by being able to look at the next word. *Intuition*: I think this is a way to predict the last token of a filter window rather than the center one.  \n",
    "\n",
    "After the GLU activation and before the residual connection, we calculate & apply attention. This uses the encoded representations (\"conved\" & \"combined\" vectors), the token embedding (only for the current word) and output from the conv layer (GLU activated).\n",
    "\n",
    "Attention is calculated by first projecting the conv layer output from hid_dim to emb_dim using a linear layer. Then the token embedding is summed into it via a residual connection. This combination is then used as the \"query\" vector to see how much it \"matches\" against the \"key\" vector which is the \"conved\" vector from the encoder. This produces a weighted vector over the src_len that sums to 1. The weighted vector is then used to compute a weighted sum over the \"value\" vectors that are the \"combined\" vectors produced by the encoder. Why this separation between the \"key\" & \"value\" vectors? The paper argues that the encoded \"conved\" is good for getting a larger context over the encoded sequence, whereas the encoded \"combined\" has more information about the specific token (because the token embedding is summed in) and is therefore more useful for making a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k27h99v_vyk5"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, kernel_size, \n",
    "               dropout_p, trg_pad_idx, device, max_length = 100):\n",
    "    super().__init__()\n",
    "\n",
    "    self.kernel_size = kernel_size\n",
    "    self.trg_pad_idx = trg_pad_idx\n",
    "    self.device = device\n",
    "\n",
    "    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "\n",
    "    self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "    self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "    \n",
    "    self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "    self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "\n",
    "    # Should the next 2 be layer specific or shared?? Try layer specific.\n",
    "    self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "    self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "\n",
    "    self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "\n",
    "    self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim,\n",
    "                                          out_channels = 2*hid_dim,\n",
    "                                          kernel_size = kernel_size)\n",
    "                                for _ in range(n_layers)])\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, trg, encoder_conved, encoder_combined):\n",
    "    # trg: [batch_sz, trg_len]\n",
    "    # encoder_conved & encoder_combined: [batch_sz, src_len, emb_dim]\n",
    "\n",
    "    batch_sz = trg.shape[0]\n",
    "    trg_len = trg.shape[1]\n",
    "\n",
    "    # create a positional tensor\n",
    "    pos = torch.arange(0, trg_len).unsqueeze(0)\\\n",
    "                      .repeat(batch_sz, 1).to(self.device) # [batch_sz, trg_len]\n",
    "\n",
    "    tok_embedded = self.tok_embedding(trg) # [batch_sz, trg_len, emb_dim]\n",
    "    pos_embedded = self.pos_embedding(pos) # [batch_sz, trg_len, emb_dim]\n",
    "\n",
    "    embedded = self.dropout(tok_embedded + pos_embedded) # [batch_sz, trg_len, emb_dim]\n",
    "\n",
    "    # project embedding to hid_dim size to pass through conv blocks\n",
    "    conv_input = self.emb2hid(embedded) # [batch_sz, trg_len, hid_dim]\n",
    "\n",
    "    # permute to convolve over the target length\n",
    "    conv_input = conv_input.permute(0,2,1) # [batch_sz, hid_dim, trg_len]\n",
    "\n",
    "    batch_sz = conv_input.shape[0]\n",
    "    hid_dim = conv_input.shape[1]\n",
    "\n",
    "    for i, conv in enumerate(self.convs):\n",
    "      conv_input = self.dropout(conv_input) # [batch_sz, trg_len, hid_dim]\n",
    "      # create a padded to prefix the sequence. this prevents the decoder from looking \"forward\"\n",
    "      padding = torch.zeros(batch_sz, hid_dim, self.kernel_size-1)\\\n",
    "                  .fill_(self.trg_pad_idx).to(self.device) # [batch_sz, hid_dim, kerne_size-1]\n",
    "\n",
    "      padded_conv_input = torch.cat((padding, conv_input), \n",
    "                                    dim=2) # [batch_sz, hid_dim, trg_len + kerne_size-1]\n",
    "\n",
    "      conved = conv(padded_conv_input) # [batch_sz, 2*hid_dim, trg_len]\n",
    "      conved = F.glu(conved, dim=1) # [batch_sz, hid_dim, trg_len]\n",
    "\n",
    "      attention, conved = self.calculate_attention(embedded, conved,\n",
    "                                                   encoder_conved, encoder_combined)\n",
    "\n",
    "      # attention: [batch_sz, trg_len, src_len]\n",
    "      # conved: [batch_sz, trg_len, hid_dim]\n",
    "\n",
    "      # apply a residual connection\n",
    "      conved = (conved + conv_input) * self.scale # [batch_sz, trg_len, hid_dim]\n",
    "\n",
    "      conv_input = conved\n",
    "\n",
    "    # permute back \n",
    "    conved = conved.permute(0,2,1) # [batch_sz, trg_len, hid_dim]\n",
    "    conved = self.hid2emb(conved) # [batch_sz, trg_len, emb_dim]\n",
    "\n",
    "    output = self.fc_out(self.dropout(conved)) # [batch_sz, trg_len, output_dim]\n",
    "    \n",
    "    return output, attention\n",
    "\n",
    "  def calculate_attention(self, embedded, conved, \n",
    "                          encoder_conved, encoder_combined):\n",
    "    # embedded: [batch_sz, trg_len, emb_dim]\n",
    "    # conved: [batch_sz, hid_dim, trg_len]\n",
    "    # encoder_conved & encoder_combined: [batch_sz, src_len, emb_dim]\n",
    "\n",
    "    # permute & project down to emb_dim\n",
    "    conved_emb = self.attn_hid2emb(conved.permute(0,2,1)) # [batch_sz, trg_len, emb_dim]\n",
    "\n",
    "    combined = (conved_emb + embedded) * self.scale # [batch_sz, trg_len, emb_dim]\n",
    "\n",
    "    energy = torch.matmul(combined, encoder_conved.permute(0,2,1)) # [batch_sz, trg_len, src_len]\n",
    "\n",
    "    attention = F.softmax(energy, dim=2) # [batch_sz, trg_len, src_len]\n",
    "\n",
    "    attended_encoding = torch.matmul(attention, encoder_combined) # [batch_sz, trg_len, emb_dim]\n",
    "\n",
    "    attended_encoding = self.attn_emb2hid(attended_encoding) # [batch_sz, trg_len, hid_dim]\n",
    "\n",
    "    # apply residual connection \n",
    "    attended_combined = (conved + attended_encoding.permute(0,2,1)) * self.scale # [batch_sz,hid_dim, trg_len]\n",
    "\n",
    "    return attention, attended_combined \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1TLTblyWPwb"
   },
   "source": [
    "## Seq2Seq Model\n",
    "\n",
    "We do not pass the <eos> token to the decoder (not sure why it matters?). Encoding is similar, except that we receive a encoder_conved and an encoder_combined context vector. Since the decoding is all convolutions and in parallel there is no decoding loop. We pad the target at the end to prevent it from peeking forward. This also means that we cannot do \"teacher-forcing\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-itONFukULK"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def forward(self, src, trg):\n",
    "    # src: [batch_sz, src_len]\n",
    "    # trg: [batch_sz, trg_len - 1] # <eos> token chopped off the end\n",
    "\n",
    "    encoder_conved, encoder_combined = self.encoder(src)\n",
    "    # encoder_conved & encoder_combined: [batch_sz, src_len, emb_dim]\n",
    "\n",
    "    # Calculate predictions of next words\n",
    "    output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "    # output: [batch_sz, trg_len - 1, output_dim]\n",
    "    # output is a batch of predictions for each word in the trg sentence\n",
    "    # attention: [batch_sz, trg_len - 1, src_len]\n",
    "    # attention is a batch of attention scores across the src sentence for each word in the trg sentence\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3rzzvegmCbx"
   },
   "source": [
    "### Training the Seq2Seq Model\n",
    "\n",
    "Rest of the training is similar to all the previous tuts. In the paper they find that it is more beneficial to use a small filter (kernel of 3) and a high number of layers (5+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--Vch3gNnCOk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 37,351,429 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_LAYERS = 10 # number of conv blocks.\n",
    "DEC_LAYERS = 10\n",
    "ENC_KERNEL_SZ = 3 # must be odd\n",
    "DEC_KERNEL_SZ = 3 # can be odd/even\n",
    "DROPOUT_P = 0.25\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SZ,\n",
    "              DROPOUT_P, device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SZ,\n",
    "              DROPOUT_P, TRG_PAD_IDX, device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "# define a function that will calculate the number of trainable parameters in the model.\n",
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters.\")\n",
    "\n",
    "# function to tell us how long an epoch takes.\n",
    "def epoch_time(start_time, end_time):\n",
    "  elapsed_time = end_time - start_time\n",
    "  elapsed_mins = int(elapsed_time / 60)\n",
    "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "  return elapsed_mins, elapsed_secs \n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# The CrossEntropyLoss function calculates both the log softmax as well \n",
    "# as the negative log-likelihood of our predictions.\n",
    "# Our loss function calculates the average loss per token, however by passing\n",
    "# the index of the <pad> token as the ignore_index argument we ignore the loss \n",
    "# whenever the target token is a padding token.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hq5rYANhpsWx"
   },
   "source": [
    "We strip off the <eos> token before passing the trg into the decoder. We hope the decoder will learn to predict the <eos> token. In the RNN based models we did this by stopping the decoder loop right before the <eos> token. The expected decoder output should be $ [y_1, y_2, y_3, eos]$ and we pass this to the loss function along with $trg[1:] = [x_1, x_2, x_3, eos]$ (aka strip the <sos> token).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSimhj2TqmjI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 42s\n",
      "\tTrain Loss: 4.422 | Train PPL:  83.236\n",
      "\t Val. Loss: 3.020 |  Val. PPL:  20.492\n",
      "Epoch: 02 | Time: 1m 42s\n",
      "\tTrain Loss: 3.057 | Train PPL:  21.271\n",
      "\t Val. Loss: 2.356 |  Val. PPL:  10.552\n",
      "Epoch: 03 | Time: 1m 42s\n",
      "\tTrain Loss: 2.620 | Train PPL:  13.738\n",
      "\t Val. Loss: 2.107 |  Val. PPL:   8.225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b12078e109e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m   \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b12078e109e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# -1 to chop off <eos>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# output: [batch_sz, trg_len, out_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c25df596d142>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Calculate predictions of next words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_conved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# output: [batch_sz, trg_len - 1, output_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# output is a batch of predictions for each word in the trg sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a4d877954d6c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, trg, encoder_conved, encoder_combined)\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0;31m# create a padded to prefix the sequence. this prevents the decoder from looking \"forward\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                   \u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_sz, hid_dim, kerne_size-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       padded_conv_input = torch.cat((padding, conv_input), \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  for i, batch in enumerate(iterator):\n",
    "    src = batch.src\n",
    "    trg = batch.trg\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(src, trg[:,:-1]) # -1 to chop off <eos>\n",
    "    # output: [batch_sz, trg_len, out_dim]\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    # Flatten out across the batch to compute loss (possibly helps with parallelization?)\n",
    "    output = output.contiguous().view(-1, output_dim) # [batch_sz*trg_len-1, out_dim]\n",
    "    trg = trg[:,1:].contiguous().view(-1) # [batch_sz*trg_len-1] # 1: to chop off <sos>\n",
    "\n",
    "    loss = criterion(output, trg)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss/len(iterator)\n",
    "\n",
    "# Evaluation loop is same as training, just without gradients\n",
    "def evaluate(model, iterator, criterion):\n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(iterator):\n",
    "      src = batch.src\n",
    "      trg = batch.trg\n",
    "      output, _ = model(src, trg[:,:-1])\n",
    "      output_dim = output.shape[-1]\n",
    "\n",
    "      output = output.contiguous().view(-1, output_dim) # [batch_sz*trg_len-1, out_dim]\n",
    "      trg = trg[:,1:].contiguous().view(-1) # [batch_sz*trg_len-1] # 1: to chop off <sos>\n",
    "\n",
    "      loss = criterion(output, trg)\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss/len(iterator)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 0.1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  start_time = time.time()\n",
    "\n",
    "  train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "  valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "  if valid_loss < best_valid_loss:\n",
    "    best_valid_loss = best_valid_loss\n",
    "    torch.save(model.state_dict(), sandbox_path + 'tut5-model.pt')\n",
    "\n",
    "  print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\") \n",
    "  print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "  print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mq9J1Mqv9sX"
   },
   "outputs": [],
   "source": [
    "# Load the best model with best validation and run the test set\n",
    "model.load_state_dict(torch.load(sandbox_path + 'tut5-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pt5YHoJ0yiwY"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Decoding is done sequentially in a loop during inference.\n",
    "\n",
    "Steps:\n",
    "- Tokenize the source sentence, append <sos> & <eos>.\n",
    "- Numercalize, convert to a tensor & add a batch_dim.\n",
    "- Feed src_sent to encoder.\n",
    "- Initialize a list to hold the output sentence with a <sos> token.\n",
    "- While we have not hit max length:\n",
    "  - Convert current output sentence prediction into a tensor with a batch_dim.\n",
    "  - Pass to decoder with 2 encoder context outputs and get next out prediction.\n",
    "  - Add prediction to current output sentence.\n",
    "  - Break if prediction was an <eos>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eo6vCknv0BZf"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
    "  model.eval()\n",
    "\n",
    "  if isinstance(sentence, str):\n",
    "    np = spacy.load('de')\n",
    "    tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "  else:\n",
    "    tokens = [token.lower() for token in sentence]\n",
    "\n",
    "  tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "  src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "  src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    encoder_conved, encoder_combined = model.encoder(src_tensor)\n",
    "\n",
    "  trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "  for i in range(max_len):\n",
    "    trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "      output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\n",
    "\n",
    "    pred_token = output.argmax(2)[:,-1].item()\n",
    "\n",
    "    trg_indexes.append(pred_token)\n",
    "\n",
    "    if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "      break\n",
    "\n",
    "  trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "  return trg_tokens[1:], attention\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(111)\n",
    "\n",
    "  # attention: [batch_sz, trg_len - 1, src_len]\n",
    "  # assumes batch sz is 1 so squeeze it out.\n",
    "  attention = attention.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "  cax = ax.matshow(attention, cmap='bone')\n",
    "\n",
    "  ax.tick_params(labelsize=15)\n",
    "  ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'],\n",
    "                     rotation=45)\n",
    "  ax.set_yticklabels([''] + translation)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "      \n",
    "  plt.show()\n",
    "  plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfU-BopGaNNz"
   },
   "outputs": [],
   "source": [
    "example_idx = 2\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')\n",
    "\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "print(f'predicted trg = {translation}')\n",
    "\n",
    "display_attention(src, translation, attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wes_EH9AEae8"
   },
   "outputs": [],
   "source": [
    "\n",
    "example_idx = 2\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "print(f'predicted trg = {translation}')\n",
    "\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYQDJZmiErn1"
   },
   "source": [
    "## BLEU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qM3nSTQsEtnC"
   },
   "outputs": [],
   "source": [
    "#from torchtext.data.metrics import bleu_score\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len=50):\n",
    "  trgs = []\n",
    "  pred_trgs = []\n",
    "\n",
    "  for datum in data:\n",
    "    src = vars(datum)['src']\n",
    "    trg = vars(datum)['trg']\n",
    "\n",
    "    pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "    # cut off <eos>\n",
    "    pred_trg = pred_trg[:-1]\n",
    "\n",
    "    pred_trgs.append(pred_trg)\n",
    "    trgs.append([trg])\n",
    "\n",
    "  return bleu_score(pred_trgs, trgs)\n",
    "\n",
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f\"BLEU score = {bleu_score*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM10HBZSnpYDOLex/9J/1xD",
   "collapsed_sections": [],
   "name": "NLP-bentrevett-5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
