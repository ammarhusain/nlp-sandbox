{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20209 training images\n",
      "Found 2000 validation images\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loading checkpoint: checkpoints/fcn32s_15.pkl\n",
      "Saved epoch loss: -3.6297646389862915e+29, time: 635.1497781276703\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "285it [08:56,  1.89s/it]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import data_loader\n",
    "import models\n",
    "from optimizer import get_optimizer\n",
    "from loss.cross_entropy_2d import CrossEntropy2d\n",
    "from utils.metrics import MetricsComp, AverageComp\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "def train(config, writer=None):\n",
    "    # setup seeds\n",
    "    torch.manual_seed(config[\"seed\"])\n",
    "    torch.cuda.manual_seed(config[\"seed\"])\n",
    "    np.random.seed(config[\"seed\"])\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # setup augmentations\n",
    "    ## ah todo\n",
    "    \n",
    "    # setup dataloader\n",
    "    mit_data = data_loader.get_loader(\"mit_sceneparsing_benchmark\")\n",
    "    train_data_raw = mit_data(\"training\")\n",
    "    train_data = data.DataLoader(train_data_raw, batch_size=config[\"batch_sz\"],\\\n",
    "                                 num_workers=config[\"num_workers\"], shuffle=True)\n",
    "    \n",
    "    val_data = data.DataLoader(mit_data(\"validation\"), batch_size=config[\"batch_sz\"],\\\n",
    "                                 num_workers=config[\"num_workers\"])\n",
    "\n",
    "    # setup metrics\n",
    "    metrics_comp = MetricsComp(train_data_raw.n_classes)\n",
    "    val_loss_avg_comp = AverageComp()\n",
    "    \n",
    "    # setup model\n",
    "    model = models.get_model(\"fcn32s\", train_data_raw.n_classes).to(device)\n",
    "    #model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    \n",
    "    # setup optimizer\n",
    "    optimizer = get_optimizer(\"adam\")(model.parameters())\n",
    "    \n",
    "    print(f\"Optimizer: {optimizer}\")\n",
    "    \n",
    "    # setup learning rate scheduler (optim.lr_scheduler)\n",
    "    ## ah todo\n",
    "    loss_fn = CrossEntropy2d()\n",
    "    \n",
    "    writer.add_scalar(\"batch_size\", config[\"batch_sz\"])\n",
    "\n",
    "    i = 0\n",
    "    # Load a saved checkpoint\n",
    "    if config.get(\"resume_ckpoint\") is not None:\n",
    "        if os.path.isfile(config[\"resume_ckpoint\"]):\n",
    "            print(f\"Loading checkpoint: {config['resume_ckpoint']}\")\n",
    "            ckpoint = torch.load(config[\"resume_ckpoint\"])\n",
    "            model.load_state_dict(ckpoint[\"model_state\"])\n",
    "            optimizer.load_state_dict(ckpoint[\"optimizer_state\"])\n",
    "            i = ckpoint[\"epoch\"]\n",
    "            print(f\"Saved epoch loss: {ckpoint['epoch_loss']}, time: {ckpoint['epoch_time']}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Unable to load saved checkpoint!, Quitting\")\n",
    "            \n",
    "    while i < config[\"epochs\"]:\n",
    "        i += 1\n",
    "        epoch_loss = 0\n",
    "        epoch_time = 0\n",
    "        print(f\"Epoch {i}\")\n",
    "        for b_i, (images, labels) in tqdm(enumerate(train_data)):\n",
    "            # Free memory\n",
    "            torch.cuda.empty_cache()\n",
    "            start_ts = time.time()\n",
    "            model.train()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            out = model(images) # [batch_sz, n_classes, H=512, W=512]\n",
    "                       \n",
    "            loss = loss_fn(out, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += float(loss.item())\n",
    "            epoch_time += time.time() - start_ts\n",
    "            #print(f\"Batch Loss : {loss} ... time : {time.time() - start_ts}\")\n",
    "            writer.add_scalar(f\"{i}_batch_loss\", loss.item(), b_i)\n",
    "            # tmp\n",
    "            images.detach()\n",
    "            labels.detach()\n",
    "        # Run through validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            ctr = 0\n",
    "            for _, (images_val, labels_val) in tqdm(enumerate(val_data)):\n",
    "                if ctr > 10:\n",
    "                    break\n",
    "                ctr += 1\n",
    "                images_val = images_val.to(device)\n",
    "                labels_val = labels_val.to(device)               \n",
    "                out = model(images_val) # [batch_sz, n_classes, H=512, W=512]\n",
    "                val_loss = loss_fn(out, labels_val)\n",
    "                val_loss_avg_comp.update(val_loss.item())\n",
    "                \n",
    "                _, pred = out.max(1)\n",
    "                metrics_comp.update(label_trues=labels_val.cpu().numpy(), label_preds=pred.cpu().numpy())\n",
    "        # Add validation results to tensorboard writer\n",
    "        writer.add_scalar(\"val_loss\", val_loss_avg_comp.avg, i)\n",
    "        overall_scores, class_iou = metrics_comp.get_results()\n",
    "        print(f\"Scores after epoch {i}: {overall_scores}\")\n",
    "        for k, v in overall_scores.items():\n",
    "            writer.add_scalar(f\"val_metrics/{k}\", v, i)\n",
    "        for k, v in class_iou.items():\n",
    "            writer.add_scalar(f\"val_metrics/cls_iou_{k}\", v, i)\n",
    "        \n",
    "        # Save the model checkpoint\n",
    "        ckpoint = {\"epoch\":i,\n",
    "                   \"model_state\": model.state_dict(),\n",
    "                   \"optimizer_state\": optimizer.state_dict(),\n",
    "                   \"epoch_loss\": epoch_loss,\n",
    "                   \"epoch_time\": epoch_time,\n",
    "                  }\n",
    "        ckpoint_name = f\"checkpoints/{config['exp_name']}_{i}.pkl\"\n",
    "        torch.save(ckpoint, ckpoint_name)\n",
    "    \n",
    "            \n",
    "config = {\"exp_name\": \"fcn32s\", \"batch_sz\": 8, \"epochs\": 30,\\\n",
    "          \"seed\": 42, \"num_workers\": 1, \"resume_ckpoint\": \"checkpoints/fcn32s_15.pkl\"}\n",
    "writer = SummaryWriter(log_dir=\"checkpoints/logs\")\n",
    "\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "train(config, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh checkpoints/logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
